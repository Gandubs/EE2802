-- Assignment 1: K-Nearest Neighbors (KNN) Classification
I implemented a KNN classifier from scratch and applied it to a synthetic dataset generated using four bivariate Gaussians. The goal was to create a 2-class classification problem and explore how different factors affect KNN performance. I built a custom KNeighbourClassifier class with fit and predict methods, and used cross-validation to find the best value of k. I also studied how the training set size and data distribution (Gaussian vs Laplacian) influenced performance, compared L1 and L2 distance metrics, visualized decision boundaries, and finally applied the model to MNIST digit classification, analyzing the results using a confusion matrix.

-- Assignment 2: Regression Analysis
This assignment focused on linear regression and its variants. I started by exploring the error surface of least squares regression on a height-weight dataset. To understand model complexity and overfitting, I tried polynomial regression with increasing degrees. I also implemented different kernel methods like polynomial, Gaussian, and sigmoid kernels, and added online learning using stochastic gradient descent. To study the bias-variance trade-off, I used regularization on various datasets. The final part included Bayesian regression with MAP estimation and sequential updates.

-- Assignment 3: Classification Methods
I implemented several classification techniques ranging from linear models to probabilistic approaches. This included least squares classification using different label encodings, Linear Discriminant Analysis (LDA), and Fisher’s Discriminant for dimensionality reduction. I also worked on the perceptron algorithm, including how it behaves with outliers and on non-linearly separable data like XOR. I explored MAP classifiers with both linear and quadratic decision boundaries, logistic regression using IRLS, and wrapped up with Gaussian Mixture Models (GMMs) using the EM algorithm, experimenting with different initializations and covariance structures.

-- Assignment 4: Neural Networks
In the final assignment, I built a 3-layer feed-forward neural network (784→512→512→10) for MNIST digit classification. I implemented backpropagation and trained the model using SGD, and compared it with the Adam optimizer. I tested different activation functions like Sigmoid, Tanh, ReLU, and Leaky ReLU. To prevent overfitting, I added regularization techniques such as weight decay, dropout, and early stopping. I also compared neural networks with linear classifiers on both linearly and non-linearly separable datasets, visualizing the decision boundaries and internal activations.
